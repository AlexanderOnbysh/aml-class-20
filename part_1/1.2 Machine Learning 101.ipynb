{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Machine Learning 101"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Machine Learning as Function Approximation\n",
    "\n",
    "$\\textbf{x}$... vectorial description of the object you want to make predictions for.\n",
    "\n",
    "$y$... target value; numeric (regression), mutually exclusive class-label (classification).\n",
    "\n",
    "$h$... hypothesis (aka model); function that $h: \\textbf{x} \\mapsto y$.\n",
    "\n",
    "Goal: find $h$ from *example* pairs of $(\\textbf{x}, y)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import utils\n",
    "X, y = utils.generate_curve_data()\n",
    "fig = utils.plot_data(X, y, fn=utils.true_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Function Approximation\n",
    "\n",
    "Can we recover the grund truth: `f(x) = cos(1.5 * pi * x)` from just the datapoints? \n",
    "\n",
    "Hypothesis class: straight line, `h(x) = slope * x + intercept`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X, y = utils.generate_curve_data()\n",
    "fig = utils.plot_data(X, y, fn=utils.true_fn)\n",
    "x_plot = np.linspace(0, 1, 100)\n",
    "plt.plot(x_plot, (x_plot * 0.0 ) + 0.0, label='A')\n",
    "plt.plot(x_plot, (x_plot * -3.5 ) + 1.25, label='B')\n",
    "plt.plot(x_plot, (x_plot * 1.75 ) - 2.0, label='C')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Which Model to Select?\n",
    "\n",
    "Which of the lines `A`, `B`, or `C` is the best model?\n",
    "\n",
    "In ML we look for the one with the lowest *generalization error*\n",
    "\n",
    "\n",
    "$ G = \\int_ \\! E(h^*(\\mathbf{x}), y) \\,P(\\mathbf{x}, y)\\mathrm{dx}.$\n",
    "\n",
    "\n",
    "\n",
    "Which we cannot measure as $P(\\mathbf{x}, y)$ is unknown. \n",
    "We only have access to a sample that is drawn i.i.d. from the distribution.\n",
    "\n",
    "Different *inductive principles* can help us here:\n",
    "  * Empirical Risk Minimization\n",
    "  * Structural Risk Minimization\n",
    "  * ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Curve fitting with scikit-learn\n",
    "\n",
    "```python\n",
    "class Estimator(BaseEstimator):\n",
    "    \n",
    "    def __init__(self, a_hyper_parameter=0):\n",
    "        \"\"\"Constructor sets the estimators hyper-parameters. \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        a_hyper_parameter : int\n",
    "            A parameter that controls how the estimator is fit.\n",
    "        \"\"\"\n",
    "        self.a_hyper_parameter = a_hyper_parameter\n",
    "  \n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Fits estimator's parameters (mind the underscore) from X and y. \"\"\"\n",
    "        # set state of ``self``\n",
    "        self.state_ = None\n",
    "        return self\n",
    "            \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict response of ``X``. \"\"\"\n",
    "        # compute predictions ``pred``\n",
    "        return pred\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "help(LinearRegression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "est = LinearRegression()\n",
    "est.fit(X[:, np.newaxis], y)\n",
    "\n",
    "fig = utils.plot_data(X, y, fn=utils.true_fn, title='LinearRegression')\n",
    "utils.plot_estimator(est, fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "est = utils.PolynomialRegression(degree=15)\n",
    "est.fit(X[:, np.newaxis], y)\n",
    "\n",
    "fig = utils.plot_data(X, y, fn=utils.true_fn, title='PolynomialRegression degree=15')\n",
    "utils.plot_estimator(est, fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "est = utils.PolynomialRegression(degree=4)\n",
    "est.fit(X[:, np.newaxis], y)\n",
    "\n",
    "fig = utils.plot_data(X, y, fn=utils.true_fn, title='PolynomialRegression degree=4')\n",
    "utils.plot_estimator(est, fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ML as Search in Model Space\n",
    "\n",
    "<img src=\"img/eslii-mdl-search.png\">\n",
    "<div style=\"text-align: right\">Source: T. Hastie et al. (2017) \"Elements of Statistical Learning (Ed. 2)\"</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bias and Variance\n",
    "\n",
    "<img src=\"img/fortmannroe-bias-var.png\">\n",
    "<div style=\"text-align: right\">Source: Scott Fortmann-Roe (2012) \"Understanding the Bias-Variance Tradeoff\"</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bias and Variance - Error Decomposition\n",
    "\n",
    "<img src=\"img/fortmannroe-bias-var-decomp.png\">\n",
    "<div style=\"text-align: right\">Source: Scott Fortmann-Roe (2012) \"Understanding the Bias-Variance Tradeoff\"</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Underfitting vs Overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Fit degrees 1 to 20 on 25 random datasets, record train-test error\n",
    "res = []\n",
    "degrees = list(range(1, 20))\n",
    "for degree in degrees:\n",
    "    for rep in range(25):\n",
    "        X, y = utils.generate_curve_data(n_samples=80, seed=rep)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X[:, np.newaxis], y, test_size=.2, random_state=0)\n",
    "        est = utils.PolynomialRegression(degree=degree).fit(X_train, y_train)\n",
    "        res.append({'train_err': np.sqrt(mean_squared_error(y_train, est.predict(X_train))),\n",
    "                    'test_err': np.sqrt(mean_squared_error(y_test, est.predict(X_test))),\n",
    "                    'degree': degree,\n",
    "                    'rep': rep})\n",
    "        \n",
    "err = pd.DataFrame(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set()\n",
    "_ = sns.lineplot(x='degree', y='train_err', data=err)\n",
    "ax = sns.lineplot(x='degree', y='test_err', data=err)\n",
    "_ = ax.set(xlabel='model complexity (degree)', ylabel='error (rmse)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "aml",
   "language": "python",
   "name": "aml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
