{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bayesian Hyper-parameter Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Prospecting for Gold...\n",
    "\n",
    "<img src=\"img/daniel_krige.jpg\" style=\"float: left;\">\n",
    "<img src=\"img/kriging.png\" style=\"float: right;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Problem Statement\n",
    "\n",
    "We are interested in solving\n",
    "\n",
    "$ x^* = \\underset{x}{\\mathrm{argmin}} f(x)  $\n",
    "\n",
    "under the constraints that\n",
    "\n",
    "$ f $ is a black box for which no closed form is known (nor its gradients);\n",
    " \n",
    "$ f $ is expensive to evaluate;\n",
    "\n",
    "and evaluations of $ y = f(x) $ may be noisy.\n",
    "\n",
    "**Finding**: Under these constraints, *Bayesian Optimization* is a good fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bayesian Optimization Loop\n",
    "\n",
    "For $ t = 1 : T $:\n",
    "\n",
    "  1. Given observations $ (x_i, y_i = f(x_i)) $ for $i = 1 : t $, build a probability model (aka *surrogate model*) for the objective $f$ .\n",
    "  2. Optimize a cheap acquisition/utility function $u$ based on the posterior distribution of the surrogate model for sampling the next point:\n",
    "  \n",
    "  $ x_{t+1} = \\underset{x}{\\mathrm{argmin}} \\ u(x) $\n",
    "  \n",
    "  Surrogate model needs to provide *uncertainty* estimates to balance exploration against exploitation.\n",
    "  \n",
    "  3. Sample the next observation $ y_{t+1} = f(x_{t+1})$\n",
    "  \n",
    "  \n",
    "This procedure will build a sequence of models that model the relationship between the hyper-parameters and the performance metric hence its also called **Sequential Model-based Optimization**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Acquisition Functions\n",
    "Acquisition functions $u(x)$ specify which sample $x$ should be tried next:\n",
    "\n",
    "  * Lower confidence bound: $\\mu_{SM} + \\kappa \\sigma_{SM}$\n",
    "  * Expected improvement\n",
    "  * Probability of improvement\n",
    "\n",
    "In most cases, acquisition functions provide knobs (e.g., $\\kappa$) for controlling the exploration-exploitation trade-off. \n",
    "  - Search in regions where $\\mu_{SM}$ is high (exploitation) \n",
    "  - Probe regions where uncertainty $\\kappa \\sigma_{SM}$ is high (exploration)\n",
    "  \n",
    "We can use the acquisition function to encode additional constraints.\n",
    "  - Time to evaluate the objective $f$ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Surrogate Model\n",
    "\n",
    "### Gaussian Process Regression\n",
    "\n",
    "<img src=\"img/sm-gp.png\" width=300 style=\"float: right;\">\n",
    "\n",
    "  * Can provide a full posterior (not just $\\mu_{GP}$ and $\\sigma_{GP}$)\n",
    "  * RBF kernel well suited for smooth functions\n",
    "  * Can learn from few data points\n",
    "<div style=\"clear: both;\"/>\n",
    "\n",
    "### Tree-ensembles\n",
    "\n",
    "<img src=\"img/sm-gbrt.png\" width=300 style=\"float: right;\">\n",
    "\n",
    "  * GBM requires quantile regression to provide uncertainty estimate $\\sigma$ \n",
    "  * RandomForest uses variance of leaves for $\\sigma$ \n",
    "  * Tree methods better suited for discrete parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Toy Problem\n",
    "\n",
    "<img src=\"img/skopt-true-fn.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Toy Problem cont'\n",
    "\n",
    "Left column shows the true function (unknown), sampled points, and current model.\n",
    "\n",
    "Right column shows the value of the utility function and the next point to sample.\n",
    "<img src=\"img/skopt-steps.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bayesian Optimization with [Scikit-optimize](https://scikit-optimize.github.io/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "\n",
    "from skopt.benchmarks import branin as branin\n",
    "import utils\n",
    "\n",
    "utils.plot_branin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from skopt.plots import plot_evaluations\n",
    "from skopt import forest_minimize\n",
    "\n",
    "\n",
    "bounds = [(-5.0, 10.0), (0.0, 15.0)]\n",
    "n_calls = 160\n",
    "\n",
    "forest_res = forest_minimize(branin, bounds, n_calls=n_calls, base_estimator=\"ET\",\n",
    "                             random_state=4)\n",
    "\n",
    "_ = plot_evaluations(forest_res, bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from skopt.plots import plot_objective\n",
    "\n",
    "# plot partial-dependency between the objective function and each dimension\n",
    "_ = plot_objective(forest_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# BayesSearchCV\n",
    "\n",
    "`skopt` provides an implementation of sklearn's `BaseSearchCV` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = load_digits(10, True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.75, test_size=.25, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "print('sklearn', sklearn.__version__)\n",
    "import skopt\n",
    "print('skopt', skopt.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# log-uniform: understand as search over p = exp(x) by varying x\n",
    "opt = BayesSearchCV(\n",
    "    SVC(),\n",
    "    {\n",
    "        'C': Real(1e-6, 1e+6, prior='log-uniform'),  \n",
    "        'gamma': Real(1e-6, 1e+1, prior='log-uniform'),\n",
    "        'degree': Integer(1, 8),  # integer valued parameter\n",
    "        'kernel': Categorical(['linear', 'poly', 'rbf']),  # categorical parameter\n",
    "    },\n",
    "    optimizer_kwargs={'base_estimator': 'RF'},\n",
    "    n_iter=32,\n",
    "    cv=3,\n",
    "    random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "opt.fit(X_train, y_train)\n",
    "\n",
    "print(\"val. score: %s\" % opt.best_score_)\n",
    "print(\"test score: %s\" % opt.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "utils.plot_sklearn_tree(opt.optimizers_[0].models[-1].estimators_[0],\n",
    "    ['C', 'degree', 'gamma', 'kernel__linear', 'kernel__poly', 'kernel__rbf'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Exercise 2.2.1\n",
    "\n",
    "Run `BayesSearchCV` on the `boston_housing` dataset. \n",
    "\n",
    "Use the same hyper-parameter ranges that we used in 1.4 for `RandomizedSearchCV`.\n",
    "\n",
    "### Note\n",
    "\n",
    "`BayesSearchCV` doesnt take `scipy.stats` distribution objects; distributions need to be encoded either as tuples `(start, end, distribution_type)` or a list. \n",
    "\n",
    "Example: `(1, 100, 'log-uniform')`\n",
    "\n",
    "Display the searched points with pandas:\n",
    "```python\n",
    "gs = BayesSearchCV(..).fit(..)\n",
    "pd.DataFrame(gs.cv_results_)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "from skopt import BayesSearchCV\n",
    "\n",
    "data = load_boston()\n",
    "X = data.data\n",
    "y = data.target\n",
    "X = MinMaxScaler().fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.75, test_size=.25, random_state=0)\n",
    "\n",
    "# your code goes here \n",
    "\n",
    "print(\"best params: {}\".format(gs.best_params_))\n",
    "print(\"model score: %.3f\" % mean_absolute_error(y_test, gs.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Advanced features of `skopt`\n",
    "\n",
    "### Acquisition function taking into account function eval time\n",
    "`acq_func='EIps'` will build two models, one surrogate model for the objective function and one model to predict the time a function evaluation takes. This is taken into account where to probe/search next.\n",
    "\n",
    "### Parallelization\n",
    "Due to the sequential nature of the Bayesian Optimization loop parallelization is tricky. `skopt` allows you to probe multiple points per iteration. \n",
    "\n",
    "### Callbacks &  Check-pointing\n",
    "Callbacks can be used to implement check-pointing (pause/resume) and interrupts. \n",
    "\n",
    "```python\n",
    "def on_step(optim_result):\n",
    "    skopt.dump(optim_result, 'result.pkl')\n",
    "\n",
    "searchcv = BayesSearchCV(...)\n",
    "searchcv.fit(X, y, callback=on_step)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# When to use BO over RS/GS?\n",
    "\n",
    "Bayesian Optimization usually yields better final performance than Random Search.\n",
    "\n",
    "Anytime performance of BO not necessarily better than RS.\n",
    "\n",
    "\n",
    "<div  style=\"float: right;\" >\n",
    "<img src=\"img/automl-bo-rs.png\">\n",
    "<div style=\"text-align: right\">Source: A. Biedenkapp, \"BOHB Robust and Efficient Hyperparameter Optimization at Scale.\"</div>\n",
    "</div>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "  * Hyper-parameter tuning makes a significant difference in accuracy of trained models.\n",
    "  * 20-10 years ago hyper-parameter tuning was considerend an art/craft:\n",
    "    - Neural Networks: nr of layers, hidden units, activation functions, optimizer settings, ...\n",
    "    - Part of the coming-of-age of an ML practioneer.\n",
    "  * Now, the tools exist to make some of this knowledge obsolete / less important.\n",
    "  * (Bayesian) Hyper-parameter optimization is the most practical form of AML that is readily available today.\n",
    "    - Many open source solutions exist:\n",
    "      - [spearmint](https://github.com/HIPS/Spearmint)\n",
    "      - [hyperopt](http://hyperopt.github.io/hyperopt/)\n",
    "      - [skopt](https://scikit-optimize.github.io/)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "aml",
   "language": "python",
   "name": "aml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
